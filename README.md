# Segment Anything with Inpainting Models

This project explores the comparison between OpenAI's inpainting model and Stable Diffusion's inpainting capabilities. The workflow begins with the use of the **Segment Anything Model (SAM)** to generate segment masks for selected image samples. These masks are then utilized to perform inpainting using the two models, and the results are compared.

## Features

- **Segment Anything Model (SAM):** Automatically generates segment masks for images.
- **OpenAI Inpainting Model:** Applies inpainting to masked regions using OpenAI's model.
- **Stable Diffusion Inpainting:** Performs inpainting using Stable Diffusion's capabilities.
- **Comparison of Results:** Visual and qualitative comparison of the outputs from both models.

## Workflow

1. **Segment Mask Generation:**  
    Use SAM to generate segment masks for the input images.

2. **Inpainting with OpenAI Model:**  
    Apply OpenAI's inpainting model to the masked regions.

3. **Inpainting with Stable Diffusion:**  
    Use Stable Diffusion to perform inpainting on the same masked regions.

4. **Comparison:**  
    Compare the results from both models to evaluate their performance.

## Requirements

- Python 3.8+
- PyTorch
- OpenAI API access
- Stable Diffusion setup
- Segment Anything Model (SAM) dependencies

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/your-username/segment-inpainting-comparison.git
    cd segment-inpainting-comparison
    ```

2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Set up API keys and model weights as required.

## Usage
Each of the implementations include the code for Segment Anything. Images loading currently hardcoded into the code but can be passed as command line arguments in future implementation. 

1. Perform inpainting with OpenAI's model:
    ```bash
    python gpt.py
    ```

2. Perform inpainting with Stable Diffusion:
    ```bash
    python stable_diff.py 
    ```
3. Perform inpainting with Stable Diffusion (lightweight implementation):
    ```bash
    python lightweight.py 
    ```

Ignore utils.py for now, I will later check why I have that

TODO 
- "Caching" and loading segment masks generated by SAM to prevent running the model each time for the same image. 
- Quantized versions of Stable Diffusion are available (see Huggingface documentation), but is currently not implemented
    for lack of availability on Apple's MPS chips. 
    - if running on Colab, adapt code (on lightweight.py) to allow 4 or 8-bit quantized versions